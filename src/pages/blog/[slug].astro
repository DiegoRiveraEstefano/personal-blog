---
import BlogLayout from '../../layouts/BlogLayout.astro';

export async function getStaticPaths() {
  const posts = [
    {
      slug: 'django-rest-framework-guide',
      title: 'Building Scalable APIs with Django REST Framework',
      description: 'Learn how to create robust and scalable APIs using Django REST Framework, including authentication, pagination, and best practices for production deployment.',
      date: '2024-01-15',
      readTime: '8 min read',
      tags: ['Django', 'Python', 'API'],
      author: 'Django Expert',
      content: `
        <h2>Introduction to Django REST Framework</h2>
        <p>Django REST Framework (DRF) is a powerful and flexible toolkit for building Web APIs in Django. It provides a comprehensive set of tools and conventions that make it easy to build APIs that are both robust and maintainable. In this comprehensive guide, we'll explore the key concepts and best practices for building scalable APIs.</p>
        
        <blockquote>
          "Django REST Framework is the de facto standard for building APIs in the Django ecosystem, trusted by thousands of developers worldwide."
        </blockquote>
        
        <h3>Why Choose Django REST Framework?</h3>
        <p>DRF offers several advantages over building APIs from scratch:</p>
        <ul>
          <li><strong>Serialization:</strong> Powerful serialization system for converting complex data types</li>
          <li><strong>Authentication:</strong> Multiple authentication methods out of the box</li>
          <li><strong>Permissions:</strong> Flexible permission system for securing endpoints</li>
          <li><strong>Viewsets:</strong> Class-based views that reduce code duplication</li>
          <li><strong>Browsable API:</strong> Interactive web interface for testing endpoints</li>
        </ul>
        
        <h3>Setting Up Your First API</h3>
        <p>Let's start by installing Django REST Framework and setting up a basic project structure.</p>
        
        <pre><code># Install Django and DRF
pip install django djangorestframework

# Create a new Django project
django-admin startproject myapi
cd myapi

# Create a new app
python manage.py startapp blog</code></pre>
        
        <p>Add 'rest_framework' to your INSTALLED_APPS in settings.py:</p>
        
        <pre><code>INSTALLED_APPS = [
    'django.contrib.admin',
    'django.contrib.auth',
    'django.contrib.contenttypes',
    'django.contrib.sessions',
    'django.contrib.messages',
    'django.contrib.staticfiles',
    'rest_framework',
    'blog',
]

# Add DRF settings
REST_FRAMEWORK = {
    'DEFAULT_PAGINATION_CLASS': 'rest_framework.pagination.PageNumberPagination',
    'PAGE_SIZE': 20,
    'DEFAULT_AUTHENTICATION_CLASSES': [
        'rest_framework.authentication.SessionAuthentication',
        'rest_framework.authentication.TokenAuthentication',
    ],
    'DEFAULT_PERMISSION_CLASSES': [
        'rest_framework.permissions.IsAuthenticatedOrReadOnly',
    ],
}</code></pre>
        
        <h3>Creating Models and Serializers</h3>
        <p>Let's create a simple blog model and its corresponding serializer:</p>
        
        <pre><code># models.py
from django.db import models
from django.contrib.auth.models import User

class Category(models.Model):
    name = models.CharField(max_length=100)
    slug = models.SlugField(unique=True)
    created_at = models.DateTimeField(auto_now_add=True)
    
    class Meta:
        verbose_name_plural = "categories"
    
    def __str__(self):
        return self.name

class Article(models.Model):
    title = models.CharField(max_length=200)
    slug = models.SlugField(unique=True)
    content = models.TextField()
    author = models.ForeignKey(User, on_delete=models.CASCADE)
    category = models.ForeignKey(Category, on_delete=models.CASCADE)
    published = models.BooleanField(default=False)
    created_at = models.DateTimeField(auto_now_add=True)
    updated_at = models.DateTimeField(auto_now=True)
    
    class Meta:
        ordering = ['-created_at']
    
    def __str__(self):
        return self.title</code></pre>
        
        <p>Now let's create serializers for our models:</p>
        
        <pre><code># serializers.py
from rest_framework import serializers
from .models import Article, Category

class CategorySerializer(serializers.ModelSerializer):
    class Meta:
        model = Category
        fields = ['id', 'name', 'slug', 'created_at']

class ArticleSerializer(serializers.ModelSerializer):
    author = serializers.StringRelatedField(read_only=True)
    category = CategorySerializer(read_only=True)
    category_id = serializers.IntegerField(write_only=True)
    
    class Meta:
        model = Article
        fields = [
            'id', 'title', 'slug', 'content', 'author', 
            'category', 'category_id', 'published', 
            'created_at', 'updated_at'
        ]
        read_only_fields = ['author', 'created_at', 'updated_at']
    
    def create(self, validated_data):
        validated_data['author'] = self.context['request'].user
        return super().create(validated_data)</code></pre>
        
        <h3>Building ViewSets and URLs</h3>
        <p>ViewSets provide a way to group related views together and automatically generate URL patterns:</p>
        
        <pre><code># views.py
from rest_framework import viewsets, permissions, filters
from rest_framework.decorators import action
from rest_framework.response import Response
from django_filters.rest_framework import DjangoFilterBackend
from .models import Article, Category
from .serializers import ArticleSerializer, CategorySerializer

class CategoryViewSet(viewsets.ModelViewSet):
    queryset = Category.objects.all()
    serializer_class = CategorySerializer
    lookup_field = 'slug'

class ArticleViewSet(viewsets.ModelViewSet):
    queryset = Article.objects.filter(published=True)
    serializer_class = ArticleSerializer
    lookup_field = 'slug'
    filter_backends = [DjangoFilterBackend, filters.SearchFilter, filters.OrderingFilter]
    filterset_fields = ['category', 'author']
    search_fields = ['title', 'content']
    ordering_fields = ['created_at', 'updated_at']
    ordering = ['-created_at']
    
    def get_permissions(self):
        if self.action in ['create', 'update', 'partial_update', 'destroy']:
            permission_classes = [permissions.IsAuthenticated]
        else:
            permission_classes = [permissions.AllowAny]
        return [permission() for permission in permission_classes]
    
    @action(detail=False, methods=['get'])
    def featured(self, request):
        featured_articles = self.queryset.filter(featured=True)[:5]
        serializer = self.get_serializer(featured_articles, many=True)
        return Response(serializer.data)</code></pre>
        
        <p>Configure URLs to use the ViewSets:</p>
        
        <pre><code># urls.py
from django.contrib import admin
from django.urls import path, include
from rest_framework.routers import DefaultRouter
from blog.views import ArticleViewSet, CategoryViewSet

router = DefaultRouter()
router.register(r'articles', ArticleViewSet)
router.register(r'categories', CategoryViewSet)

urlpatterns = [
    path('admin/', admin.site.urls),
    path('api/', include(router.urls)),
    path('api-auth/', include('rest_framework.urls')),
]</code></pre>
        
        <h3>Authentication and Permissions</h3>
        <p>DRF provides several authentication methods. Here's how to implement token authentication:</p>
        
        <pre><code># Add to settings.py
INSTALLED_APPS = [
    # ... other apps
    'rest_framework.authtoken',
]

# Create tokens for existing users
python manage.py drf_create_token &lt;username&gt;</code></pre>
        
        <p>Create a custom permission class for more granular control:</p>
        
        <pre><code># permissions.py
from rest_framework import permissions

class IsAuthorOrReadOnly(permissions.BasePermission):
    def has_object_permission(self, request, view, obj):
        # Read permissions for any request
        if request.method in permissions.SAFE_METHODS:
            return True
        
        # Write permissions only for the author
        return obj.author == request.user</code></pre>
        
        <h3>Testing Your API</h3>
        <p>DRF provides excellent testing tools. Here's how to write comprehensive tests:</p>
        
        <pre><code># tests.py
from django.test import TestCase
from django.contrib.auth.models import User
from rest_framework.test import APIClient
from rest_framework import status
from .models import Article, Category

class ArticleAPITestCase(TestCase):
    def setUp(self):
        self.client = APIClient()
        self.user = User.objects.create_user(
            username='testuser',
            password='testpass123'
        )
        self.category = Category.objects.create(
            name='Technology',
            slug='technology'
        )
        
    def test_create_article(self):
        self.client.force_authenticate(user=self.user)
        data = {
            'title': 'Test Article',
            'slug': 'test-article',
            'content': 'This is a test article.',
            'category_id': self.category.id,
            'published': True
        }
        response = self.client.post('/api/articles/', data)
        self.assertEqual(response.status_code, status.HTTP_201_CREATED)
        
    def test_list_articles(self):
        response = self.client.get('/api/articles/')
        self.assertEqual(response.status_code, status.HTTP_200_OK)</code></pre>
        
        <h3>Performance Optimization</h3>
        <p>For production APIs, consider these optimization techniques:</p>
        
        <h4>Database Optimization</h4>
        <pre><code># Use select_related and prefetch_related
class ArticleViewSet(viewsets.ModelViewSet):
    def get_queryset(self):
        return Article.objects.select_related('author', 'category').filter(published=True)</code></pre>
        
        <h4>Caching</h4>
        <pre><code># Add caching to settings.py
CACHES = {
    'default': {
        'BACKEND': 'django.core.cache.backends.redis.RedisCache',
        'LOCATION': 'redis://127.0.0.1:6379/1',
    }
}

# Use cache in views
from django.core.cache import cache

class ArticleViewSet(viewsets.ModelViewSet):
    def list(self, request, *args, **kwargs):
        cache_key = 'articles_list'
        cached_data = cache.get(cache_key)
        
        if cached_data is None:
            response = super().list(request, *args, **kwargs)
            cache.set(cache_key, response.data, 300)  # Cache for 5 minutes
            return response
        
        return Response(cached_data)</code></pre>
        
        <h3>Deployment Considerations</h3>
        <p>When deploying your DRF API to production, consider these important factors:</p>
        
        <ul>
          <li><strong>Security:</strong> Use HTTPS, secure secret keys, and proper CORS settings</li>
          <li><strong>Database:</strong> Use PostgreSQL or MySQL for production</li>
          <li><strong>Static Files:</strong> Serve static files through a CDN</li>
          <li><strong>Monitoring:</strong> Implement logging and error tracking</li>
          <li><strong>Rate Limiting:</strong> Protect against abuse with rate limiting</li>
        </ul>
        
        <h3>Conclusion</h3>
        <p>Django REST Framework provides a robust foundation for building scalable APIs. By following the patterns and best practices outlined in this guide, you'll be able to create APIs that are not only functional but also maintainable and performant. Remember to always consider security, testing, and performance optimization as integral parts of your development process.</p>
        
        <p>The key to mastering DRF is practice and understanding the underlying principles. Start with simple APIs and gradually incorporate more advanced features as your applications grow in complexity.</p>
      `
    },
    {
      slug: 'javascript-es2024-features',
      title: 'Modern JavaScript: ES2024 Features You Should Know',
      description: 'Explore the latest JavaScript features including array grouping, temporal API, and other ES2024 additions that will improve your development workflow.',
      date: '2024-01-10',
      readTime: '6 min read',
      tags: ['JavaScript', 'ES2024'],
      author: 'JS Developer',
      content: `
        <h2>What's New in ES2024</h2>
        <p>ECMAScript 2024 brings several exciting features that enhance developer productivity and code readability. These additions continue JavaScript's evolution toward a more powerful and expressive language. Let's explore the most important features that you should start using today.</p>
        
        <h3>Array Grouping with Object.groupBy()</h3>
        <p>One of the most anticipated features is the ability to group array elements efficiently. The new <code>Object.groupBy()</code> method makes data manipulation much more intuitive:</p>
        
        <pre><code>const users = [
  { name: 'Alice', age: 25, department: 'Engineering' },
  { name: 'Bob', age: 30, department: 'Marketing' },
  { name: 'Charlie', age: 35, department: 'Engineering' },
  { name: 'Diana', age: 28, department: 'Design' },
  { name: 'Eve', age: 32, department: 'Marketing' }
];

// Group by department
const byDepartment = Object.groupBy(users, user => user.department);
console.log(byDepartment);
/*
{
  Engineering: [
    { name: 'Alice', age: 25, department: 'Engineering' },
    { name: 'Charlie', age: 35, department: 'Engineering' }
  ],
  Marketing: [
    { name: 'Bob', age: 30, department: 'Marketing' },
    { name: 'Eve', age: 32, department: 'Marketing' }
  ],
  Design: [
    { name: 'Diana', age: 28, department: 'Design' }
  ]
}
*/

// Group by age ranges
const byAgeGroup = Object.groupBy(users, user => {
  if (user.age < 30) return 'young';
  if (user.age < 35) return 'middle';
  return 'senior';
});

console.log(byAgeGroup);</code></pre>
        
        <h3>Map.groupBy() for Advanced Grouping</h3>
        <p>For cases where you need non-string keys, <code>Map.groupBy()</code> provides the same functionality but returns a Map:</p>
        
        <pre><code>const products = [
  { name: 'Laptop', price: 999, category: 'Electronics' },
  { name: 'Book', price: 15, category: 'Education' },
  { name: 'Phone', price: 699, category: 'Electronics' },
  { name: 'Desk', price: 299, category: 'Furniture' }
];

// Group by price ranges using Map
const priceRanges = new Map([
  ['budget', (price) => price < 100],
  ['mid-range', (price) => price >= 100 && price < 500],
  ['premium', (price) => price >= 500]
]);

const byPriceRange = Map.groupBy(products, product => {
  for (const [range, condition] of priceRanges) {
    if (condition(product.price)) return range;
  }
  return 'unknown';
});

console.log(byPriceRange);</code></pre>
        
        <h3>Promise.withResolvers()</h3>
        <p>This new method provides a cleaner way to create promises with external resolve/reject handlers:</p>
        
        <pre><code>// Old way - creating promises with external handlers
let resolvePromise, rejectPromise;
const promise = new Promise((resolve, reject) => {
  resolvePromise = resolve;
  rejectPromise = reject;
});

// New way - much cleaner!
const { promise, resolve, reject } = Promise.withResolvers();

// Example: Creating a timeout utility
function createTimeout(ms) {
  const { promise, resolve } = Promise.withResolvers();
  
  setTimeout(() => {
    resolve(`Timeout completed after ${ms}ms`);
  }, ms);
  
  return promise;
}

// Usage
createTimeout(1000).then(console.log); // 'Timeout completed after 1000ms'

// Example: Manual promise control
const { promise: userInput, resolve: resolveInput } = Promise.withResolvers();

// Simulate user interaction
document.getElementById('submit-btn').addEventListener('click', () => {
  const value = document.getElementById('input').value;
  resolveInput(value);
});

userInput.then(value => {
  console.log('User entered:', value);
});</code></pre>
        
        <h3>Atomic Operations: waitAsync</h3>
        <p>For applications using SharedArrayBuffer, the new <code>Atomics.waitAsync()</code> provides non-blocking wait operations:</p>
        
        <pre><code>// Create a shared buffer
const sab = new SharedArrayBuffer(1024);
const int32 = new Int32Array(sab);

// Non-blocking wait - returns a promise
const result = Atomics.waitAsync(int32, 0, 0, 1000);

if (result.async) {
  result.value.then(status => {
    console.log('Wait completed with status:', status);
    // status can be 'ok', 'not-equal', or 'timed-out'
  });
} else {
  console.log('Wait completed immediately:', result.value);
}

// In another thread or worker, you can notify:
// Atomics.notify(int32, 0, 1);</code></pre>
        
        <h3>Regular Expression v Flag</h3>
        <p>The new <code>v</code> flag enhances Unicode support in regular expressions:</p>
        
        <pre><code>// Enhanced Unicode property support
const emojiRegex = /\p{Emoji}/v;
console.log(emojiRegex.test('Hello ðŸ‘‹')); // true

// Set notation and string literals in character classes
const mathSymbols = /[\p{Math}--[+\-*/]]/v;
console.log(mathSymbols.test('âˆ‘')); // true
console.log(mathSymbols.test('+')); // false (excluded)

// Case-insensitive sets
const vowels = /[AEIOUaeiou]/v;
const consonants = /[\p{Letter}--[AEIOUaeiou]]/iv;

console.log(consonants.test('B')); // true
console.log(consonants.test('b')); // true
console.log(consonants.test('A')); // false</code></pre>
        
        <h3>ArrayBuffer Transfer and Resize</h3>
        <p>New methods for more efficient memory management:</p>
        
        <pre><code>// Transferring ArrayBuffer ownership
const buffer1 = new ArrayBuffer(1024);
const view1 = new Uint8Array(buffer1);
view1[0] = 42;

// Transfer ownership (buffer1 becomes detached)
const buffer2 = buffer1.transfer();
console.log(buffer1.byteLength); // 0 (detached)
console.log(buffer2.byteLength); // 1024

// Resize ArrayBuffer (if resizable)
const resizableBuffer = new ArrayBuffer(1024, { maxByteLength: 2048 });
console.log(resizableBuffer.byteLength); // 1024

resizableBuffer.resize(1536);
console.log(resizableBuffer.byteLength); // 1536</code></pre>
        
        <h3>Practical Examples and Use Cases</h3>
        
        <h4>Data Processing Pipeline</h4>
        <pre><code>// Combining multiple ES2024 features
async function processUserData(users) {
  // Group users by department
  const departments = Object.groupBy(users, user => user.department);
  
  // Process each department asynchronously
  const { promise: allProcessed, resolve } = Promise.withResolvers();
  const results = new Map();
  
  let completed = 0;
  const totalDepartments = Object.keys(departments).length;
  
  for (const [dept, deptUsers] of Object.entries(departments)) {
    // Simulate async processing
    setTimeout(() => {
      const processed = deptUsers.map(user => ({
        ...user,
        processed: true,
        processedAt: new Date().toISOString()
      }));
      
      results.set(dept, processed);
      completed++;
      
      if (completed === totalDepartments) {
        resolve(results);
      }
    }, Math.random() * 1000);
  }
  
  return allProcessed;
}

// Usage
const users = [
  { name: 'Alice', department: 'Engineering' },
  { name: 'Bob', department: 'Marketing' },
  { name: 'Charlie', department: 'Engineering' }
];

processUserData(users).then(results => {
  console.log('All departments processed:', results);
});</code></pre>
        
        <h4>Real-time Data Aggregation</h4>
        <pre><code>class DataAggregator {
  constructor() {
    this.data = [];
    this.subscribers = new Set();
  }
  
  addData(item) {
    this.data.push({ ...item, timestamp: Date.now() });
    this.notifySubscribers();
  }
  
  subscribe() {
    const { promise, resolve } = Promise.withResolvers();
    this.subscribers.add(resolve);
    return promise;
  }
  
  notifySubscribers() {
    const grouped = Object.groupBy(this.data, item => item.category);
    
    this.subscribers.forEach(resolve => {
      resolve(grouped);
    });
    
    this.subscribers.clear();
  }
  
  getAggregatedData() {
    return Object.groupBy(this.data, item => {
      const hour = new Date(item.timestamp).getHours();
      return `${hour}:00-${hour + 1}:00`;
    });
  }
}

// Usage
const aggregator = new DataAggregator();

aggregator.subscribe().then(data => {
  console.log('Received aggregated data:', data);
});

aggregator.addData({ category: 'sales', value: 100 });
aggregator.addData({ category: 'marketing', value: 50 });</code></pre>
        
        <h3>Browser Support and Polyfills</h3>
        <p>While ES2024 features are being implemented across browsers, you can use polyfills for broader compatibility:</p>
        
        <pre><code>// Polyfill for Object.groupBy
if (!Object.groupBy) {
  Object.groupBy = function(items, keySelector) {
    const result = {};
    
    for (const item of items) {
      const key = keySelector(item);
      if (!result[key]) {
        result[key] = [];
      }
      result[key].push(item);
    }
    
    return result;
  };
}

// Feature detection
const hasGroupBy = typeof Object.groupBy === 'function';
const hasWithResolvers = typeof Promise.withResolvers === 'function';

console.log('ES2024 support:', { hasGroupBy, hasWithResolvers });</code></pre>
        
        <h3>Migration Tips</h3>
        <p>When adopting ES2024 features in existing codebases:</p>
        
        <ul>
          <li><strong>Gradual adoption:</strong> Start with non-critical features and test thoroughly</li>
          <li><strong>Transpilation:</strong> Use Babel or TypeScript for broader browser support</li>
          <li><strong>Feature detection:</strong> Always check for feature availability before use</li>
          <li><strong>Fallbacks:</strong> Provide polyfills or alternative implementations</li>
        </ul>
        
        <h3>Conclusion</h3>
        <p>ES2024 continues JavaScript's evolution with practical features that solve real-world problems. The array grouping methods alone will simplify countless data processing tasks, while Promise.withResolvers() provides cleaner asynchronous code patterns.</p>
        
        <p>Start experimenting with these features in your projects today. Even if you need to use polyfills for production, familiarizing yourself with the new APIs will prepare you for the future of JavaScript development.</p>
        
        <p>The JavaScript ecosystem continues to mature, and these additions demonstrate the language's commitment to developer experience and performance. Keep an eye on the TC39 proposals for what's coming next!</p>
      `
    },
    {
      slug: 'python-performance-optimization',
      title: 'Python Performance Optimization Techniques',
      description: 'Discover advanced techniques to optimize Python code performance, from algorithmic improvements to using compiled extensions and profiling tools.',
      date: '2024-01-05',
      readTime: '12 min read',
      tags: ['Python', 'Performance'],
      author: 'Python Expert',
      content: `
        <h2>Introduction to Python Performance</h2>
        <p>Python's simplicity and readability make it a favorite among developers, but its interpreted nature can sometimes lead to performance bottlenecks. However, with the right techniques and tools, you can significantly optimize your Python applications. This comprehensive guide covers everything from basic optimizations to advanced techniques used in production systems.</p>
        
        <blockquote>
          "Premature optimization is the root of all evil, but when performance matters, knowing how to optimize Python code effectively is crucial."
        </blockquote>
        
        <h3>Profiling: Measure Before You Optimize</h3>
        <p>The first rule of optimization is to measure and identify bottlenecks. Python provides several excellent profiling tools:</p>
        
        <h4>Using cProfile</h4>
        <pre><code>import cProfile
import pstats

def slow_function():
    total = 0
    for i in range(1000000):
        total += i * i
    return total

def another_function():
    return [x**2 for x in range(100000)]

# Profile your code
if __name__ == "__main__":
    profiler = cProfile.Profile()
    profiler.enable()
    
    # Your code here
    result1 = slow_function()
    result2 = another_function()
    
    profiler.disable()
    
    # Analyze results
    stats = pstats.Stats(profiler)
    stats.sort_stats('cumulative')
    stats.print_stats(10)  # Top 10 functions</code></pre>
        
        <h4>Line-by-Line Profiling with line_profiler</h4>
        <pre><code># Install: pip install line_profiler
# Usage: kernprof -l -v script.py

@profile
def optimize_me():
    # Line 1: Expensive operation
    data = [i**2 for i in range(100000)]
    
    # Line 2: Another expensive operation
    filtered = [x for x in data if x % 2 == 0]
    
    # Line 3: Final processing
    return sum(filtered)

# Run with: kernprof -l -v your_script.py</code></pre>
        
        <h3>Algorithmic Optimizations</h3>
        <p>Often, the biggest performance gains come from choosing better algorithms and data structures:</p>
        
        <h4>Choosing the Right Data Structure</h4>
        <pre><code>import time
from collections import deque, defaultdict

# List vs Deque for queue operations
def compare_queue_operations():
    # Using list (inefficient for queue)
    start = time.time()
    queue_list = []
    for i in range(100000):
        queue_list.append(i)
    for i in range(50000):
        queue_list.pop(0)  # O(n) operation!
    list_time = time.time() - start
    
    # Using deque (efficient for queue)
    start = time.time()
    queue_deque = deque()
    for i in range(100000):
        queue_deque.append(i)
    for i in range(50000):
        queue_deque.popleft()  # O(1) operation!
    deque_time = time.time() - start
    
    print(f"List time: {list_time:.4f}s")
    print(f"Deque time: {deque_time:.4f}s")
    print(f"Deque is {list_time/deque_time:.1f}x faster")

# Dictionary lookups vs list searches
def compare_lookups():
    data_list = list(range(10000))
    data_dict = {i: i for i in range(10000)}
    data_set = set(range(10000))
    
    search_items = [100, 5000, 9999]
    
    # List search O(n)
    start = time.time()
    for item in search_items * 1000:
        item in data_list
    list_time = time.time() - start
    
    # Dict/Set lookup O(1)
    start = time.time()
    for item in search_items * 1000:
        item in data_dict
    dict_time = time.time() - start
    
    print(f"List search: {list_time:.4f}s")
    print(f"Dict lookup: {dict_time:.4f}s")</code></pre>
        
        <h4>Efficient String Operations</h4>
        <pre><code># Inefficient string concatenation
def slow_string_concat(items):
    result = ""
    for item in items:
        result += str(item) + ", "
    return result[:-2]

# Efficient string concatenation
def fast_string_concat(items):
    return ", ".join(str(item) for item in items)

# String formatting comparison
def compare_string_formatting():
    name = "Alice"
    age = 30
    
    # Old style (slower)
    result1 = "Name: %s, Age: %d" % (name, age)
    
    # .format() method
    result2 = "Name: {}, Age: {}".format(name, age)
    
    # f-strings (fastest)
    result3 = f"Name: {name}, Age: {age}"
    
    return result3  # Use f-strings when possible</code></pre>
        
        <h3>NumPy for Numerical Computing</h3>
        <p>For numerical operations, NumPy can provide massive speedups:</p>
        
        <pre><code>import numpy as np
import time

def compare_numerical_operations():
    # Pure Python
    python_list = list(range(1000000))
    
    start = time.time()
    python_result = [x * 2 + 1 for x in python_list]
    python_time = time.time() - start
    
    # NumPy
    numpy_array = np.arange(1000000)
    
    start = time.time()
    numpy_result = numpy_array * 2 + 1
    numpy_time = time.time() - start
    
    print(f"Python time: {python_time:.4f}s")
    print(f"NumPy time: {numpy_time:.4f}s")
    print(f"NumPy is {python_time/numpy_time:.1f}x faster")

# Vectorized operations
def vectorized_example():
    # Instead of loops
    def slow_distance_calculation(points1, points2):
        distances = []
        for p1, p2 in zip(points1, points2):
            dist = ((p1[0] - p2[0])**2 + (p1[1] - p2[1])**2)**0.5
            distances.append(dist)
        return distances
    
    # Use NumPy vectorization
    def fast_distance_calculation(points1, points2):
        p1 = np.array(points1)
        p2 = np.array(points2)
        return np.sqrt(np.sum((p1 - p2)**2, axis=1))
    
    # Generate test data
    points1 = [(i, i+1) for i in range(100000)]
    points2 = [(i+0.5, i+1.5) for i in range(100000)]
    
    # Compare performance
    start = time.time()
    slow_result = slow_distance_calculation(points1, points2)
    slow_time = time.time() - start
    
    start = time.time()
    fast_result = fast_distance_calculation(points1, points2)
    fast_time = time.time() - start
    
    print(f"Loop-based: {slow_time:.4f}s")
    print(f"Vectorized: {fast_time:.4f}s")</code></pre>
        
        <h3>Caching and Memoization</h3>
        <p>Cache expensive computations to avoid redundant work:</p>
        
        <pre><code>from functools import lru_cache, wraps
import time

# Simple memoization decorator
def memoize(func):
    cache = {}
    
    @wraps(func)
    def wrapper(*args, **kwargs):
        key = str(args) + str(sorted(kwargs.items()))
        if key not in cache:
            cache[key] = func(*args, **kwargs)
        return cache[key]
    return wrapper

# Using functools.lru_cache
@lru_cache(maxsize=128)
def expensive_fibonacci(n):
    if n < 2:
        return n
    return expensive_fibonacci(n-1) + expensive_fibonacci(n-2)

# Custom cache with TTL (Time To Live)
class TTLCache:
    def __init__(self, maxsize=128, ttl=300):
        self.cache = {}
        self.maxsize = maxsize
        self.ttl = ttl
    
    def get(self, key):
        if key in self.cache:
            value, timestamp = self.cache[key]
            if time.time() - timestamp < self.ttl:
                return value
            else:
                del self.cache[key]
        return None
    
    def set(self, key, value):
        if len(self.cache) >= self.maxsize:
            # Remove oldest entry
            oldest_key = min(self.cache.keys(), 
                           key=lambda k: self.cache[k][1])
            del self.cache[oldest_key]
        
        self.cache[key] = (value, time.time())

def ttl_cache(ttl=300, maxsize=128):
    def decorator(func):
        cache = TTLCache(maxsize, ttl)
        
        @wraps(func)
        def wrapper(*args, **kwargs):
            key = str(args) + str(sorted(kwargs.items()))
            result = cache.get(key)
            
            if result is None:
                result = func(*args, **kwargs)
                cache.set(key, result)
            
            return result
        return wrapper
    return decorator

@ttl_cache(ttl=60, maxsize=100)
def expensive_api_call(endpoint):
    # Simulate expensive API call
    time.sleep(0.1)
    return f"Data from {endpoint}"</code></pre>
        
        <h3>Multiprocessing and Concurrency</h3>
        <p>Leverage multiple cores for CPU-bound tasks:</p>
        
        <pre><code>import multiprocessing as mp
import concurrent.futures
import asyncio
import aiohttp
from threading import Thread
import queue

# CPU-bound task with multiprocessing
def cpu_bound_task(n):
    """Simulate CPU-intensive work"""
    total = 0
    for i in range(n):
        total += i ** 2
    return total

def parallel_cpu_work():
    numbers = [1000000] * 8
    
    # Sequential processing
    start = time.time()
    sequential_results = [cpu_bound_task(n) for n in numbers]
    sequential_time = time.time() - start
    
    # Parallel processing
    start = time.time()
    with mp.Pool() as pool:
        parallel_results = pool.map(cpu_bound_task, numbers)
    parallel_time = time.time() - start
    
    print(f"Sequential: {sequential_time:.2f}s")
    print(f"Parallel: {parallel_time:.2f}s")
    print(f"Speedup: {sequential_time/parallel_time:.1f}x")

# I/O-bound tasks with asyncio
async def fetch_url(session, url):
    async with session.get(url) as response:
        return await response.text()

async def async_io_example():
    urls = [
        'https://httpbin.org/delay/1',
        'https://httpbin.org/delay/1',
        'https://httpbin.org/delay/1',
        'https://httpbin.org/delay/1',
    ]
    
    start = time.time()
    async with aiohttp.ClientSession() as session:
        tasks = [fetch_url(session, url) for url in urls]
        results = await asyncio.gather(*tasks)
    async_time = time.time() - start
    
    print(f"Async I/O time: {async_time:.2f}s")

# Producer-Consumer pattern with threading
class ProducerConsumer:
    def __init__(self, max_queue_size=10):
        self.queue = queue.Queue(max_queue_size)
        self.results = []
    
    def producer(self, items):
        for item in items:
            # Simulate work
            processed_item = item ** 2
            self.queue.put(processed_item)
        
        # Signal completion
        self.queue.put(None)
    
    def consumer(self):
        while True:
            item = self.queue.get()
            if item is None:
                break
            
            # Process item
            self.results.append(item * 2)
            self.queue.task_done()
    
    def run(self, data):
        producer_thread = Thread(target=self.producer, args=(data,))
        consumer_thread = Thread(target=self.consumer)
        
        producer_thread.start()
        consumer_thread.start()
        
        producer_thread.join()
        consumer_thread.join()
        
        return self.results</code></pre>
        
        <h3>Memory Optimization</h3>
        <p>Reduce memory usage with these techniques:</p>
        
        <pre><code>import sys
from array import array
import gc

# Use generators instead of lists
def memory_efficient_processing():
    # Memory-hungry approach
    def load_all_data():
        return [i**2 for i in range(1000000)]
    
    # Memory-efficient approach
    def generate_data():
        for i in range(1000000):
            yield i**2
    
    # Compare memory usage
    data_list = load_all_data()
    print(f"List size: {sys.getsizeof(data_list)} bytes")
    
    data_gen = generate_data()
    print(f"Generator size: {sys.getsizeof(data_gen)} bytes")

# Use __slots__ to reduce memory overhead
class RegularClass:
    def __init__(self, x, y):
        self.x = x
        self.y = y

class SlottedClass:
    __slots__ = ['x', 'y']
    
    def __init__(self, x, y):
        self.x = x
        self.y = y

def compare_class_memory():
    regular_objects = [RegularClass(i, i+1) for i in range(1000)]
    slotted_objects = [SlottedClass(i, i+1) for i in range(1000)]
    
    regular_size = sum(sys.getsizeof(obj.__dict__) for obj in regular_objects)
    slotted_size = sum(sys.getsizeof(obj) for obj in slotted_objects)
    
    print(f"Regular class total size: {regular_size}")
    print(f"Slotted class total size: {slotted_size}")

# Use array for numeric data
def efficient_numeric_storage():
    # Python list (more memory)
    python_list = [i for i in range(100000)]
    
    # Array (less memory)
    int_array = array('i', range(100000))
    
    print(f"List size: {sys.getsizeof(python_list)}")
    print(f"Array size: {sys.getsizeof(int_array)}")

# Memory profiling
def memory_profile_example():
    import tracemalloc
    
    tracemalloc.start()
    
    # Your memory-intensive code here
    data = [i**2 for i in range(100000)]
    more_data = {i: i**3 for i in range(50000)}
    
    current, peak = tracemalloc.get_traced_memory()
    print(f"Current memory usage: {current / 1024 / 1024:.1f} MB")
    print(f"Peak memory usage: {peak / 1024 / 1024:.1f} MB")
    
    tracemalloc.stop()</code></pre>
        
        <h3>Compiled Extensions</h3>
        <p>For ultimate performance, consider compiled extensions:</p>
        
        <h4>Using Numba for JIT Compilation</h4>
        <pre><code># pip install numba
from numba import jit, njit
import numpy as np

@jit
def slow_python_function(arr):
    total = 0
    for i in range(len(arr)):
        total += arr[i] ** 2
    return total

@njit  # No-python mode for maximum speed
def fast_numba_function(arr):
    total = 0
    for i in range(len(arr)):
        total += arr[i] ** 2
    return total

def compare_numba_performance():
    data = np.random.random(1000000)
    
    # Pure Python
    start = time.time()
    result1 = sum(x**2 for x in data)
    python_time = time.time() - start
    
    # Numba JIT
    start = time.time()
    result2 = fast_numba_function(data)
    numba_time = time.time() - start
    
    print(f"Python: {python_time:.4f}s")
    print(f"Numba: {numba_time:.4f}s")
    print(f"Speedup: {python_time/numba_time:.1f}x")</code></pre>
        
        <h4>Cython Example</h4>
        <pre><code># setup.py for Cython
from setuptools import setup
from Cython.Build import cythonize

setup(
    ext_modules = cythonize("fast_module.pyx")
)

# fast_module.pyx
def fast_sum(double[:] arr):
    cdef double total = 0
    cdef int i
    cdef int n = arr.shape[0]
    
    for i in range(n):
        total += arr[i]
    
    return total

# Build with: python setup.py build_ext --inplace</code></pre>
        
        <h3>Best Practices Summary</h3>
        
        <h4>General Optimization Guidelines</h4>
        <ul>
          <li><strong>Profile first:</strong> Always measure before optimizing</li>
          <li><strong>Algorithm choice:</strong> Often the biggest impact comes from better algorithms</li>
          <li><strong>Data structures:</strong> Choose the right tool for the job</li>
          <li><strong>Avoid premature optimization:</strong> Focus on readability first, optimize when needed</li>
          <li><strong>Use built-ins:</strong> Python's built-in functions are usually optimized</li>
        </ul>
        
        <h4>Performance Checklist</h4>
        <pre><code># Performance optimization checklist
def optimization_checklist():
    """
    1. Profile your code (cProfile, line_profiler)
    2. Choose efficient algorithms and data structures
    3. Use list comprehensions and generator expressions
    4. Leverage NumPy for numerical computations
    5. Implement caching for expensive operations
    6. Use multiprocessing for CPU-bound tasks
    7. Use asyncio for I/O-bound tasks
    8. Optimize memory usage (generators, __slots__)
    9. Consider compiled extensions (Numba, Cython)
    10. Monitor and measure improvements
    """
    pass

# Example of applying multiple optimizations
class OptimizedDataProcessor:
    def __init__(self):
        self.cache = {}
    
    @lru_cache(maxsize=1000)
    def expensive_calculation(self, x):
        # Simulate expensive computation
        return sum(i**2 for i in range(x))
    
    def process_batch(self, data):
        # Use NumPy for numerical operations
        np_data = np.array(data)
        
        # Vectorized operations
        squared = np_data ** 2
        normalized = squared / np.max(squared)
        
        return normalized.tolist()
    
    def parallel_process(self, batches):
        with concurrent.futures.ProcessPoolExecutor() as executor:
            results = list(executor.map(self.process_batch, batches))
        return results</code></pre>
        
        <h3>Conclusion</h3>
        <p>Python performance optimization is both an art and a science. The key is to understand your application's bottlenecks and apply the appropriate techniques. Start with profiling to identify hot spots, then apply optimizations systematically.</p>
        
        <p>Remember that readability and maintainability should not be sacrificed for marginal performance gains. However, when performance is critical, Python offers numerous tools and techniques to achieve the speed you need.</p>
        
        <p>The techniques covered in this guide can often provide 10x to 100x performance improvements, making Python suitable for high-performance applications when properly optimized.</p>
      `
    }
  ];
  
  return posts.map(post => ({
    params: { slug: post.slug },
    props: { post }
  }));
}

const { post } = Astro.props;
---

<BlogLayout 
  title={post.title}
  description={post.description}
  date={post.date}
  readTime={post.readTime}
  tags={post.tags}
  author={post.author}
>
  <div set:html={post.content} />
</BlogLayout>